{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up the app folder.\n",
    "create a folder called app\n",
    "add in todo.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file app already exists.\n"
     ]
    }
   ],
   "source": [
    "! cd   C:/Users/BM/Facial Recognition && mkdir app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Install Kivy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NB : this is about 150MB\n",
    "# !pip install kivy[full] kivy_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set up the validation folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy the application_data folder into app folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a custom layer module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a new file called layers.py and add this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom L1Distance Layer\n",
    "\n",
    "#Import dependancies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# L1 distance layer\n",
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bring over the PKL model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy paste siamesemodel.pkl folder into app folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import dependencies for Kivy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] [Logger      ] Record log in C:\\Users\\BM\\.kivy\\logs\\kivy_23-09-03_1.txt\n",
      "[ERROR  ] [Core        ] option --ip not recognized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kivy Usage: ipykernel_launcher.py [KIVY OPTION...] [-- PROGRAM OPTIONS]::\n",
      "\n",
      "            Options placed after a '-- ' separator, will not be touched by kivy,\n",
      "            and instead passed to your program.\n",
      "\n",
      "            Set KIVY_NO_ARGS=1 in your environment or before you import Kivy to\n",
      "            disable Kivy's argument parser.\n",
      "\n",
      "        -h, --help\n",
      "            Prints this help message.\n",
      "        -d, --debug\n",
      "            Shows debug log.\n",
      "        -a, --auto-fullscreen\n",
      "            Force 'auto' fullscreen mode (no resolution change).\n",
      "            Uses your display's resolution. This is most likely what you want.\n",
      "        -c, --config section:key[:value]\n",
      "            Set a custom [section] key=value in the configuration object.\n",
      "        -f, --fullscreen\n",
      "            Force running in fullscreen mode.\n",
      "        -k, --fake-fullscreen\n",
      "            Force 'fake' fullscreen mode (no window border/decoration).\n",
      "            Uses the resolution specified by width and height in your config.\n",
      "        -w, --windowed\n",
      "            Force running in a window.\n",
      "        -p, --provider id:provider[,options]\n",
      "            Add an input provider (eg: ccvtable1:tuio,192.168.0.1:3333).\n",
      "        -m mod, --module=mod\n",
      "            Activate a module (use \"list\" to get a list of available modules).\n",
      "        -r, --rotation\n",
      "            Rotate the window's contents (0, 90, 180, 270).\n",
      "        -s, --save\n",
      "            Save current Kivy configuration.\n",
      "        --size=640x480\n",
      "            Size of window geometry.\n",
      "        --dpi=96\n",
      "            Manually overload the Window DPI (for testing only.)\n",
      "    \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGetoptError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\kivy\\__init__.py:431\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     opts, args \u001b[39m=\u001b[39m getopt(sys_argv[\u001b[39m1\u001b[39;49m:], \u001b[39m'\u001b[39;49m\u001b[39mhp:fkawFem:sr:dc:\u001b[39;49m\u001b[39m'\u001b[39;49m, [\n\u001b[0;32m    432\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhelp\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfullscreen\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mwindowed\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfps\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mevent\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    433\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mmodule=\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msave\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfake-fullscreen\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mauto-fullscreen\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    434\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mmultiprocessing-fork\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdisplay=\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msize=\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrotate=\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    435\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mconfig=\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdebug\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdpi=\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    437\u001b[0m \u001b[39mexcept\u001b[39;00m GetoptError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\getopt.py:93\u001b[0m, in \u001b[0;36mgetopt\u001b[1;34m(args, shortopts, longopts)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m     opts, args \u001b[39m=\u001b[39m do_longs(opts, args[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m:], longopts, args[\u001b[39m1\u001b[39;49m:])\n\u001b[0;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\getopt.py:157\u001b[0m, in \u001b[0;36mdo_longs\u001b[1;34m(opts, opt, longopts, args)\u001b[0m\n\u001b[0;32m    155\u001b[0m     opt, optarg \u001b[39m=\u001b[39m opt[:i], opt[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 157\u001b[0m has_arg, opt \u001b[39m=\u001b[39m long_has_args(opt, longopts)\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m has_arg:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\getopt.py:174\u001b[0m, in \u001b[0;36mlong_has_args\u001b[1;34m(opt, longopts)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m possibilities:\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mraise\u001b[39;00m GetoptError(_(\u001b[39m'\u001b[39m\u001b[39moption --\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m not recognized\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m%\u001b[39m opt, opt)\n\u001b[0;32m    175\u001b[0m \u001b[39m# Is there an exact match?\u001b[39;00m\n",
      "\u001b[1;31mGetoptError\u001b[0m: option --ip not recognized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# import kivy dependencies \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# kivy layout \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkivy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m \u001b[39mimport\u001b[39;00m App\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkivy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39muix\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mboxlayout\u001b[39;00m \u001b[39mimport\u001b[39;00m BoxLayout\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\kivy\\__init__.py:440\u001b[0m\n\u001b[0;32m    439\u001b[0m     kivy_usage()\n\u001b[1;32m--> 440\u001b[0m     sys\u001b[39m.\u001b[39;49mexit(\u001b[39m2\u001b[39;49m)\n\u001b[0;32m    442\u001b[0m mp_fork \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2095\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2092\u001b[0m \u001b[39mif\u001b[39;00m exception_only:\n\u001b[0;32m   2093\u001b[0m     stb \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAn exception has occurred, use \u001b[39m\u001b[39m%\u001b[39m\u001b[39mtb to see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2094\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mthe full traceback.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m]\n\u001b[1;32m-> 2095\u001b[0m     stb\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mInteractiveTB\u001b[39m.\u001b[39;49mget_exception_only(etype,\n\u001b[0;32m   2096\u001b[0m                                                      value))\n\u001b[0;32m   2097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2098\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2099\u001b[0m         \u001b[39m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[0;32m   2100\u001b[0m         \u001b[39m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[0;32m   2101\u001b[0m         \u001b[39m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exception_only\u001b[39m(\u001b[39mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[39m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39;49mstructured_traceback(\u001b[39mself\u001b[39;49m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(exception[\u001b[39m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m    569\u001b[0m             etype,\n\u001b[0;32m    570\u001b[0m             evalue,\n\u001b[0;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[0;32m    573\u001b[0m             context,\n\u001b[0;32m    574\u001b[0m         )\n\u001b[0;32m    575\u001b[0m         \u001b[39m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[39m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[39mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\IPython\\core\\ultratb.py:1428\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtb \u001b[39m=\u001b[39m etb\n\u001b[1;32m-> 1428\u001b[0m \u001b[39mreturn\u001b[39;00m FormattedTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m   1429\u001b[0m     \u001b[39mself\u001b[39;49m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1430\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\IPython\\core\\ultratb.py:1319\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1316\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[0;32m   1317\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose_modes:\n\u001b[0;32m   1318\u001b[0m     \u001b[39m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1319\u001b[0m     \u001b[39mreturn\u001b[39;00m VerboseTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m   1320\u001b[0m         \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1321\u001b[0m     )\n\u001b[0;32m   1322\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMinimal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m   1323\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39mget_exception_only(\u001b[39mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\IPython\\core\\ultratb.py:1172\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructured_traceback\u001b[39m(\n\u001b[0;32m   1164\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1165\u001b[0m     etype: \u001b[39mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1169\u001b[0m     number_of_lines_of_context: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[0;32m   1170\u001b[0m ):\n\u001b[0;32m   1171\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1172\u001b[0m     formatted_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m   1173\u001b[0m                                                            tb_offset)\n\u001b[0;32m   1175\u001b[0m     colors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mColors  \u001b[39m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m     colorsnormal \u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39mNormal  \u001b[39m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\IPython\\core\\ultratb.py:1062\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tb_offset, \u001b[39mint\u001b[39m)\n\u001b[0;32m   1060\u001b[0m head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_header(\u001b[39mstr\u001b[39m(etype), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlong_header)\n\u001b[0;32m   1061\u001b[0m records \u001b[39m=\u001b[39m (\n\u001b[1;32m-> 1062\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[39mif\u001b[39;00m etb \u001b[39melse\u001b[39;00m []\n\u001b[0;32m   1063\u001b[0m )\n\u001b[0;32m   1065\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[0;32m   1066\u001b[0m skipped \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\BM\\Facial Recognition\\facialrecognition\\lib\\site-packages\\IPython\\core\\ultratb.py:1130\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mwhile\u001b[39;00m cf \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         mod \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetmodule(cf\u001b[39m.\u001b[39;49mtb_frame)\n\u001b[0;32m   1131\u001b[0m         \u001b[39mif\u001b[39;00m mod \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m             mod_name \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "# import kivy dependencies \n",
    "# kivy layout \n",
    "from kivy.app import App\n",
    "from kivy.uix.boxlayout import BoxLayout\n",
    "\n",
    "# kivy ux components \n",
    "from kivy.uix.image import Image\n",
    "from kivy.uix.button import Button\n",
    "from kivy.uix.label import Label\n",
    "\n",
    "# miscalleneous kivy stuff \n",
    "from kivy.clock import Clock\n",
    "from kivy.graphics.texture import Texture\n",
    "from kivy.logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `from kivy.app import App`: This line imports the main application class, `App`, which represents the entire layout and serves as the root of the Kivy application.\n",
    "\n",
    "- `from kivy.uix.boxlayout import BoxLayout`: This import statement brings in the `BoxLayout` class, used to create containers within our layout where other widgets can be organized.\n",
    "\n",
    "- `from kivy.uix.image import Image`: Here, we import the `Image` class, which enables us to incorporate images, including real-time webcam feeds, into our Kivy application.\n",
    "\n",
    "- `from kivy.uix.button import Button`: This line imports the `Button` class, which allows us to create interactive buttons within our application.\n",
    "\n",
    "- `from kivy.uix.label import Label`: Similarly, the `Label` class is imported to facilitate the creation of labeled text elements within the user interface.\n",
    "\n",
    "- `from kivy.clock import Clock`: This import statement brings in the `Clock` module, which helps in scheduling continuous updates to ensure we receive a real-time feed or make periodic actions.\n",
    "\n",
    "- `from kivy.graphics.texture import Texture`: This line allows us to use the `Texture` class for converting images, particularly from OpenCV, into a format suitable for display in the Kivy interface.\n",
    "\n",
    "- `from kivy.logger import Logger`: Finally, we import the `Logger` class, which can be used to view various metrics and logs related to the application's behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import other dependancies\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from layers import L1Dist\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build the layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build app and layout \n",
    "class CamApp(App):\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    CamApp().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# build app and layout \n",
    "class CamApp(App):\n",
    "\n",
    "```\n",
    "```python\n",
    "    def build(self):\n",
    "        self.img1 = Image(size_hint=(1, .8))\n",
    "        self.button = Button(text=\"Verify\", size_hint=(1, .1))\n",
    "        self.verification = Label(text=\"verification Uninitiated\", size_hint=(1,.1))\n",
    "```\n",
    "we have size hint to define how big we want the box to be .\n",
    "for image we have set Image module\n",
    "for verify button we set Button module\n",
    "for verification we set a label module\n",
    "\n",
    "```python\n",
    "        # add items to layout \n",
    "        layout = BoxLayout(orientation='vertical')\n",
    "        layout.add_widget(self.img1)\n",
    "        layout.add_widget(self.button)\n",
    "        layout.add_widget(self.verification)\n",
    "\n",
    "        return layout\n",
    "```\n",
    "we add the image, button and label to our kivy layout and then return the layout to be viewable\n",
    "the layout widgets aare arranged sequentially as appears on code\n",
    "you can now run the app bu using py facialapp.py on your terminal\n",
    "    \n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    CamApp().run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full code down here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kivy dependencies \n",
    "from kivy.app import App\n",
    "from kivy.uix.boxlayout import BoxLayout\n",
    "from kivy.uix.image import Image\n",
    "from kivy.uix.button import Button\n",
    "from kivy.uix.label import Label\n",
    "from kivy.clock import Clock\n",
    "from kivy.graphics.texture import Texture\n",
    "from kivy.logger import Logger\n",
    "\n",
    "# import other dependancies\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from layers import L1Dist\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# build app and layout \n",
    "class CamApp(App):\n",
    "\n",
    "    def build(self):\n",
    "        self.img1 = Image(size_hint=(1, .8))\n",
    "        self.button = Button(text=\"Verify\", size_hint=(1, .1))\n",
    "        self.verification = Label(text=\"verification Uninitiated\", size_hint=(1,.1))\n",
    "\n",
    "        # add items to layout \n",
    "        layout = BoxLayout(orientation='vertical')\n",
    "        layout.add_widget(self.img1)\n",
    "        layout.add_widget(self.button)\n",
    "        layout.add_widget(self.verification)\n",
    "\n",
    "        return layout\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    CamApp().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Build the update function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now create a webcam capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def build(self):\n",
    "        #.......\n",
    "\n",
    "        # setup capture\n",
    "        self.capture = cv2.VideoCapture(1)\n",
    "```\n",
    "\n",
    "```python\n",
    "    def update(self, *args):\n",
    "        \n",
    "        # read frame from opencv \n",
    "        ret, frame = self.capture.read()\n",
    "\n",
    "        # cut down frame\n",
    "        frame = frame[120:120+250,200:200+250, :]\n",
    "```\n",
    "we are capturing the camera fed from opencv and then passing it to our def update function\n",
    "```python\n",
    "\n",
    "        # flip horizontally and convert image to texture\n",
    "        buf = cv2.flip(frame, 0).tostring()\n",
    "        img_texture = Texture.create(size=(frame.shape[1], frame.shape[0]), colorfmt='bgr')\n",
    "        img_texture.blit_buffer(buf, colorfmt='bgr', bufferfmt='ubyte')\n",
    "        self.web_cam.texture = img_texture\n",
    "```\n",
    "here we are flipping our mage horizontally and convert it to a string since blitbuffer expects the data to be a bytestring. remembe rthat open cv saves an image as a numpy array\n",
    "we then convert image into a kivy image texture where we pass the size, and colorfmt\n",
    "we then set the webcam image to the img_texture\n",
    "\n",
    "```python\n",
    "        Clock.schedule_interval(self.update, 1.0/33.0)\n",
    "\n",
    "```\n",
    "we want to run our update method once every 33 seconds\n",
    "\n",
    "everything should now run as expected. if the camera feed isnt working, use the videocapture number you used in the model creation and testing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.  Bring over the preprocessing function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy over the preprocessing function and make the following changes\n",
    "\n",
    "```python\n",
    "    def preprocess(self, file_path):\n",
    "        byte_img = tf.io.read_file(file_path)\n",
    "        img = tf.io.decode_jpeg(byte_img)\n",
    "        img = tf.image.resize(img, (100, 100))\n",
    "        img = img / 255.0\n",
    "        return img\n",
    "```\n",
    "add self to the function arguments since we are inheriting from the class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Bring over the verification function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bring over the verification model and make a few changes\n",
    "\n",
    "replace other arguments with *args\n",
    "\n",
    "specify the thresholds\n",
    "\n",
    "```python\n",
    "        # specify thresholds \n",
    "        detection_threshold = 0.5\n",
    "        verification_threshold = 0.5\n",
    "```\n",
    "then we need to capture the image from our webcam feed and save it into application_data/input image ad input_image.jpg\n",
    "\n",
    "```python\n",
    "        # capture input image from our webcam \n",
    "        SAVE_PATH = os.path.join('application_data', 'input_image', 'input_image.jpg')\n",
    "        ret, frame = self.capture.read()\n",
    "        frame = frame[120:120+250,200:200+250, :]\n",
    "        cv2.imwrite(SAVE_PATH, frame)\n",
    "```\n",
    "\n",
    "then in our build function, we have to load our keras model\n",
    "```python\n",
    "\n",
    "        # load tf model\n",
    "        self.model = tf.keras.models.load_model('siamesemodel.pkl', custom_objects={'L1Dist':L1Dist})\n",
    "```\n",
    "\n",
    "full code snippet\n",
    "\n",
    "```python\n",
    "    def verify(self, *args):\n",
    "        # specify thresholds \n",
    "        detection_threshold = 0.5\n",
    "        verification_threshold = 0.5\n",
    "\n",
    "        # capture input image from our webcam \n",
    "        SAVE_PATH = os.path.join('application_data', 'input_image', 'input_image.jpg')\n",
    "        ret, frame = self.capture.read()\n",
    "        frame = frame[120:120+250,200:200+250, :]\n",
    "        cv2.imwrite(SAVE_PATH, frame)\n",
    "\n",
    "        # build results array \n",
    "        results = []\n",
    "        for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "            input_img = self.preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "            validation_img = self.preprocess(os.path.join('application_data', 'verification_images', image))\n",
    "\n",
    "            # make predictions \n",
    "            result = self.model.predict(list(np.expand_dims([input_img, validation_img], axis = 1)))\n",
    "            results.append(result)\n",
    "\n",
    "        detection = np.sum(np.array(results) > detection_threshold)\n",
    "        verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images')))\n",
    "        verified = verification > verification_threshold\n",
    "\n",
    "        return results, verified\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Update the verification function to handle new paths and save the current frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Update the verification function to set the verified text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Link the verification function to the button.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Set up the logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
